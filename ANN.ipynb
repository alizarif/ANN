{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import PyPDF2\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import faiss\n",
    "import hnswlib\n",
    "from annoy import AnnoyIndex\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2Tokenizer, GPT2Model, RobertaTokenizer, RobertaModel\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    if model_name.startswith('gpt2'):\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        model = GPT2Model.from_pretrained(model_name)\n",
    "    elif model_name.startswith('roberta'):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaModel.from_pretrained(model_name)\n",
    "    elif model_name.startswith('bert'):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "    elif model_name.startswith('bart'):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "    elif model_name.startswith('xlnet'):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() if page.extract_text() else \"\"\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_and_vectorize(tokens, model, tokenizer):\n",
    "    vectors = []\n",
    "    for token in set(tokens):\n",
    "        inputs = tokenizer(token, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors), list(set(tokens))\n",
    "\n",
    "def create_annoy_index(vectors, dimension, n_trees=10): # These are the defult values for ANNOY. You can modify them according to your needs\n",
    "    annoy_index = AnnoyIndex(dimension, 'angular')\n",
    "    for i, vector in enumerate(vectors):\n",
    "        annoy_index.add_item(i, vector)\n",
    "    annoy_index.build(n_trees)\n",
    "    return annoy_index\n",
    "\n",
    "def create_faiss_index(vectors): # These are the defult values for FAISS. You can modify them according to your needs\n",
    "    dimension = vectors.shape[1]\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)\n",
    "    faiss_index.add(vectors)\n",
    "    return faiss_index\n",
    "\n",
    "def create_hnsw_index(vectors, dimension, space='l2', M=16, ef_construction=200):  # These are the defult values for HSNW. You can modify them according to your needs\n",
    "    hnsw_index = hnswlib.Index(space=space, dim=dimension)\n",
    "    num_elements = vectors.shape[0]\n",
    "    hnsw_index.init_index(max_elements=num_elements, ef_construction=ef_construction, M=M)\n",
    "    hnsw_index.add_items(vectors)\n",
    "    hnsw_index.set_ef(10)\n",
    "    return hnsw_index\n",
    "\n",
    "def find_nearest_neighbors(query_vector, index, index_type, n_neighbors=15):\n",
    "    if index_type == 'annoy':\n",
    "        nearest_ids = index.get_nns_by_vector(query_vector, n_neighbors)\n",
    "    elif index_type == 'faiss':\n",
    "        query_vector = np.array([query_vector]).astype('float32')\n",
    "        _, nearest_ids = index.search(query_vector, n_neighbors)\n",
    "        nearest_ids = nearest_ids[0]\n",
    "    elif index_type == 'hnsw':\n",
    "        labels, _ = index.knn_query(query_vector, k=n_neighbors)\n",
    "        nearest_ids = labels[0]\n",
    "    return nearest_ids\n",
    "\n",
    "def process_files_and_save_results(pdf_filenames, model_names, target_word, pdf_directory, year):\n",
    "    results = []\n",
    "    aggregated_text = \"\"\n",
    "\n",
    "    for pdf_filename in pdf_filenames:\n",
    "        pdf_path = f'{pdf_directory}/{pdf_filename}'  \n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        aggregated_text += pdf_text\n",
    "\n",
    "    for model_name in model_names:\n",
    "        model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "        print(f\"Processing year {year} with {model_name}\")\n",
    "        tokens = preprocess_text(aggregated_text)\n",
    "        vectors, unique_tokens = tokenize_and_vectorize(tokens, model, tokenizer)\n",
    "\n",
    "        # Vectorize the target word\n",
    "        target_vector, _ = tokenize_and_vectorize([target_word], model, tokenizer)\n",
    "\n",
    "        # Get the vector dimension from the loaded model\n",
    "        dimension = model.config.hidden_size\n",
    "\n",
    "        # Create indexes for vectors\n",
    "        annoy_index = create_annoy_index(vectors, dimension)\n",
    "        faiss_index = create_faiss_index(vectors)\n",
    "        hnsw_index = create_hnsw_index(vectors, dimension)\n",
    "\n",
    "        # Now, let's find the nearest neighbors using the indexes\n",
    "        for index, index_type in zip([annoy_index, faiss_index, hnsw_index], ['annoy', 'faiss', 'hnsw']):\n",
    "            nearest_ids = find_nearest_neighbors(target_vector[0], index, index_type)\n",
    "            nearest_words = [unique_tokens[id] for id in nearest_ids]  # Get the actual nearest words\n",
    "\n",
    "            # Append results for saving\n",
    "            results.append([year, model_name, index_type, nearest_words])\n",
    "\n",
    "    # Save results to CSV and then convert to Excel\n",
    "    csv_filename = f'nearest_neighbors_results_{year}.csv'\n",
    "    with open(csv_filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Year', 'Model Used', 'Index Type', 'Nearest Neighbors'])\n",
    "        writer.writerows(results)\n",
    "\n",
    "    # Convert CSV to Excel\n",
    "    df_results = pd.read_csv(csv_filename)\n",
    "    excel_filename = csv_filename.replace('.csv', '.xlsx')\n",
    "    df_results.to_excel(excel_filename, index=False)\n",
    "\n",
    "    return excel_filename\n",
    "\n",
    "\n",
    "# Load the Excel file to get PDF filenames\n",
    "excel_path = 'files.xlsx'  # Update this path. It cosiders that you have a column named 'Year' and another column named 'Filename'\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# I had a list of PDF files from 2010 to 2023. Modify this according to your needs\n",
    "start_year = 2010 \n",
    "end_year = 2023\n",
    "\n",
    "# Define models to use\n",
    "# Based on size of memory and computational power you can selecet or deselct these models\n",
    "model_names = ['xlnet-base-cased', 'gpt2', 'gpt2-medium', 'gpt2-large', 'roberta-base', 'roberta-large', 'bert-base-uncased', 'bert-large-uncased']\n",
    "\n",
    "# Define the target word\n",
    "target_word = 'inflation'\n",
    "\n",
    "# Define the directory containing PDF files\n",
    "pdf_directory = '' # Update this path. It considers that you have PDF files in the same directory as this script. If not, you can use an absolute path like '/path/to/pdf/files\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    # Filter the DataFrame based on the year\n",
    "    df_filtered = df[df['Year'] == year]\n",
    "    pdf_filenames = df_filtered.iloc[:, 0].tolist()\n",
    "\n",
    "    # Process the PDF files and save the results\n",
    "    excel_results_filename = process_files_and_save_results(pdf_filenames, model_names, target_word, pdf_directory, year)\n",
    "    print(f\"Results for year {year} saved to {excel_results_filename}\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
